# -*- coding: utf-8 -*-
"""ECG_DataVisualisation_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LhPX7HuUYCuFckjrfTRzAv8Rkdj9ct-3

# **RESEARCH ON DATA VISUALIZATION TECHNIQUES ON ECG DATA (MIT-BIH DATASET) USING ML AND DL**

## INSTALLING LIBRARIES
"""

# Section 1: Install Necessary Libraries
!pip install numpy==1.25.2
!pip install --upgrade pandas==2.2.2
!pip install --upgrade sweetviz==2.1.4
!pip install lazypredict
!pip install scikit-learn
!pip install biosppy
!pip install peakutils
!pip install umap-learn

# Import necessary libraries
import os
import kagglehub
import pandas as pd
import numpy as np
import seaborn as sns
import shutil
import umap
import pywt
import sweetviz as sv
import matplotlib.pyplot as plt
import tensorflow as tf
from scipy.signal import welch
from IPython.display import display, HTML
from mpl_toolkits.mplot3d import Axes3D
from scipy.signal import spectrogram
from biosppy.signals import ecg
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score
from sklearn.svm import SVR
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from sklearn.ensemble import ExtraTreesClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras import models, layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from lazypredict.Supervised import LazyClassifier
from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier,
                              ExtraTreesClassifier, HistGradientBoostingClassifier)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.utils import resample

"""## DATA LOADING AND PREPROCESSING"""

# Download latest version
path = kagglehub.dataset_download("gkshitij/mit-bih-arrhythmia-database-new")

print("Path to dataset files:", path)

# Step 1: Load the Dataset
file_path = "/root/.cache/kagglehub/datasets/gkshitij/mit-bih-arrhythmia-database-new/versions/1/MIT-BIH Arrhythmia Database new.csv"  # Replace with your file path
data = pd.read_csv(file_path)

# Step 2: Data Exploration and Preprocessing
data.info()
data.describe()

# Select numeric columns
numeric_data = data.select_dtypes(include=["float64", "int64"])

# Step 2: Handle Missing Values (if any)
data = data.dropna()

# Step 3: Encode Categorical Target Variable
if 'type' not in data.columns:
    raise ValueError("Target column 'type' not found in dataset.")

label_encoder = LabelEncoder()
data['type'] = label_encoder.fit_transform(data['type'])

# Step 4: Feature Scaling
X = data.drop(columns=['type'])  # Features
y = data['type']  # Target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 5: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

print("Done preprocessing data")

# Step 6: Convert Back to DataFrame
resampled_data = pd.DataFrame(X_resampled, columns=X.columns)
resampled_data['type'] = y_resampled  # Add target column back

# Step 7: Save Processed Data as CSV
os.makedirs("/mnt/data", exist_ok=True)
resampled_data.to_csv("/mnt/data/resampled_data.csv", index=False)
print("Processed data saved as 'resampled_data.csv'.")

# Save the preprocessed (but NOT resampled) data
preprocessed_data = pd.DataFrame(X_scaled, columns=X.columns)
preprocessed_data['type'] = y  # Add original target column back
preprocessed_data.to_csv("preprocessed_data.csv", index=False)
print("Preprocessed data saved as 'preprocessed_data.csv'.")

# Save the resampled (SMOTE-applied) data
resampled_data.to_csv("resampled_data.csv", index=False)
print("Resampled data saved as 'resampled_data.csv'.")

# Step 8: Move File for Download
shutil.move("/mnt/data/resampled_data.csv", "resampled_data.csv")

print("You can now download 'resampled_data.csv'.")

"""## PERFORMING DATA VISUALISATON ON ORIGINAL DATA

### *TRADITIONAL VISUALISATION*
"""

# Assume a sampling rate (in Hz); adjust if needed
sampling_rate = 360

# Generate a time vector (dataset doesn't provide a time column)
time = np.arange(0, len(data) / sampling_rate, 1 / sampling_rate)

# Select an ECG channel for demonstration; here we use the '0_pre-RR' column
ecg_signal = data['0_pre-RR'].values

# Plot the raw ECG signal (time-domain)
plt.figure(figsize=(12, 6))
plt.plot(time, ecg_signal)
plt.title('Raw ECG Signal (Time Domain)')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude (mV)')
plt.grid(True)
plt.show()

# Process the ECG signal using the correct biosppy function (from biosppy.signals)
out = ecg.ecg(ecg_signal, sampling_rate=sampling_rate)
# The processed (filtered) ECG signal is stored in the 'filtered' key
filtered_ecg = out['filtered']

# Plot the processed (filtered) ECG signal
plt.figure(figsize=(12, 6))
plt.plot(time, filtered_ecg)
plt.title('Processed ECG Signal (Filtered)')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude (mV)')
plt.grid(True)
plt.show()

# Frequency Domain Analysis (FFT)
freq = np.fft.fftfreq(len(ecg_signal), d=1/sampling_rate)
fft_signal = np.fft.fft(ecg_signal)
plt.figure(figsize=(12, 6))
plt.plot(freq[:len(freq)//2], np.abs(fft_signal)[:len(freq)//2])
plt.title('ECG Signal in Frequency Domain')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Amplitude')
plt.grid(True)
plt.show()

# Power Spectral Density (PSD) using Welch's method
f, Pxx = welch(ecg_signal, fs=sampling_rate, nperseg=1024)
plt.figure(figsize=(12, 6))
plt.semilogy(f, Pxx)
plt.title('Power Spectral Density of ECG Signal')
plt.xlabel('Frequency (Hz)')
plt.ylabel('PSD [VÂ²/Hz]')
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def compute_fft(signal, Fs):
    """
    Compute the Fast Fourier Transform (FFT) of a time-domain signal.

    Parameters:
        signal (ndarray): 1D array containing the time-domain signal.
        Fs (float): Sampling frequency (Hz).

    Returns:
        fft_freqs (ndarray): Frequency bins (Hz).
        fft_values (ndarray): Complex FFT coefficients.
    """
    fft_values = np.fft.fft(signal)
    fft_freqs = np.fft.fftfreq(len(signal), d=1/Fs)
    return fft_freqs, fft_values

def plot_fft(fft_freqs, fft_values):
    """
    Plot the FFT amplitude spectrum for positive frequencies.

    Parameters:
        fft_freqs (ndarray): Frequency bins (Hz).
        fft_values (ndarray): Complex FFT coefficients.
    """
    # For real signals, FFT output is symmetric. We plot only the positive frequencies.
    pos_mask = fft_freqs >= 0
    freqs_positive = fft_freqs[pos_mask]
    fft_positive = fft_values[pos_mask]
    amplitude_spectrum = np.abs(fft_positive)

    # Create a figure with two subplots:
    # 1. Full spectrum plot.
    # 2. Zoomed-in plot (e.g., up to 10 Hz) to inspect lower-frequency components.
    fig, axs = plt.subplots(2, 1, figsize=(10, 10))

    # Full amplitude spectrum plot
    axs[0].plot(freqs_positive, amplitude_spectrum, 'b-', lw=2)
    axs[0].set_title('Full FFT Amplitude Spectrum')
    axs[0].set_xlabel('Frequency (Hz)')
    axs[0].set_ylabel('Amplitude')
    axs[0].grid(True)

    # Zoomed-in view (e.g., up to 10 Hz) to examine heart-rate and respiration components
    zoom_mask = freqs_positive <= 10
    axs[1].plot(freqs_positive[zoom_mask], amplitude_spectrum[zoom_mask], 'r-', lw=2)
    axs[1].set_title('Zoomed FFT Amplitude Spectrum (0-10 Hz)')
    axs[1].set_xlabel('Frequency (Hz)')
    axs[1].set_ylabel('Amplitude')
    axs[1].grid(True)

    plt.tight_layout()
    plt.show()

def main():
    # Set the sampling frequency and signal duration
    Fs = 360           # Sampling frequency (Hz)
    duration = 10      # Duration in seconds
    N = int(Fs * duration)
    t = np.linspace(0, duration, N, endpoint=False)

    # Generate a synthetic ECG-like signal over a longer period.
    # Components:
    #   - Heart rate: ~1.2 Hz (72 bpm)
    #   - Respiratory modulation: ~0.25 Hz (15 breaths/min)
    #   - High-frequency noise: 50 Hz (common interference)
    #   - Additional mid-frequency component: ~3 Hz (for ECG morphology details)
    ecg_signal = (np.sin(2 * np.pi * 1.2 * t) +         # Heart rate component
                  0.3 * np.sin(2 * np.pi * 0.25 * t) +      # Respiration component
                  0.5 * np.sin(2 * np.pi * 3 * t) +         # Additional ECG morphological component
                  0.2 * np.sin(2 * np.pi * 50 * t))         # High-frequency noise component

    # Optionally, add some random noise to mimic real ECG data variability.
    noise = 0.05 * np.random.randn(N)
    ecg_signal += noise

    # Compute the FFT of the ECG signal.
    fft_freqs, fft_values = compute_fft(ecg_signal, Fs)

    # Plot the full FFT amplitude spectrum and a zoomed view.
    plot_fft(fft_freqs, fft_values)

if __name__ == '__main__':
    main()

"""### MODERN VISUALIZATION"""

# Scatterplot for all features
plt.figure(figsize=(12, 8))  # Increase figure size
sns.scatterplot(data)  # Scatter plot
# Move legend outside the plot
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()  # Adjust layout to fit everything
plt.show()

# Boxplot for all features
plt.figure(figsize=(15, 8))  # Increase figure size
sns.boxplot(data=data)
plt.xticks(rotation=45, ha="right")  # Rotate labels and align right
plt.title("Box Plot of Features", fontsize=14)
plt.show()

# Step 1: Select only numerical columns for correlation computation
numerical_data = data.select_dtypes(include=['float64', 'int64'])

# Step 2: Compute the correlation matrix
correlation_matrix = numerical_data.corr()

# Step 3: Increase figure size for better readability
plt.figure(figsize=(16, 12))
sns.set(font_scale=1.2)  # Adjust text scaling

# Step 4: Create heatmap with better colors and readability
heatmap = sns.heatmap(
    correlation_matrix,
    annot=True, fmt=".2f", cmap="coolwarm",  # Use vibrant colors
    linewidths=0.5, linecolor="gray",  # Add cell borders
    annot_kws={"size": 8},  # Reduce annotation size
    cbar_kws={"shrink": 0.75}  # Reduce colorbar size
)

# Step 5: Improve label visibility
plt.xticks(rotation=45, ha="right")
plt.yticks(rotation=0)
plt.title("Correlation Matrix", fontsize=16)

# Step 6: Adjust layout and display
plt.tight_layout()
plt.show()

# Create pairplots
sns.pairplot(data)
plt.show()

selected_features = data[['0_pre-RR', '0_rPeak']]
sns.pairplot(selected_features)

plt.show()

# Voilineplot for all features
plt.figure(figsize=(20, 8))  # Increase width and height
sns.violinplot(data)
plt.xticks(rotation=90)  # Rotate labels for better readability
plt.tight_layout()  # Adjust layout to prevent label cutoff
plt.show()

# Define the sampling frequency
fs = 360  # MIT-BIH dataset sampling frequency

# Select only numerical ECG-related columns
numeric_columns = data.select_dtypes(include=['number']).columns

# Loop through all numerical columns and generate individual spectrograms
for column in numeric_columns:
    try:
        # Extract ECG signal for the current column
        ecg_signal = data[column].values

        # Compute spectrogram
        frequencies, times, Sxx = spectrogram(ecg_signal, fs, nperseg=256, noverlap=128)

        # Plot Spectrogram for the current column
        plt.figure(figsize=(12, 6))
        plt.pcolormesh(times, frequencies, 10 * np.log10(Sxx + 1e-10), shading='gouraud', cmap='inferno')
        plt.title(f'ECG Signal Spectrogram ({column})', fontsize=16)
        plt.ylabel('Frequency (Hz)', fontsize=14)
        plt.xlabel('Time (s)', fontsize=14)
        plt.colorbar(label='Power Spectral Density (dB/Hz)')
        plt.ylim(0, 50)
        plt.grid()
        plt.show()

    except Exception as e:
        print(f"Skipping column {column} due to error: {e}")

# Generate a combined spectrogram
combined_ecg_signal = data[numeric_columns].mean(axis=1).values  # Averaging all numerical columns

# Compute spectrogram for the combined signal
frequencies, times, Sxx = spectrogram(combined_ecg_signal, fs, nperseg=256, noverlap=128)

# Plot the single combined spectrogram
plt.figure(figsize=(12, 6))
plt.pcolormesh(times, frequencies, 10 * np.log10(Sxx + 1e-10), shading='gouraud', cmap='inferno')
plt.title('ECG Signal Spectrogram (MIT-BIH DATASET)', fontsize=16)
plt.ylabel('Frequency (Hz)', fontsize=14)
plt.xlabel('Time (s)', fontsize=14)
plt.colorbar(label='Power Spectral Density (dB/Hz)')
plt.ylim(0, 50)
plt.grid()
plt.show()

combined_signal = np.concatenate(X_scaled, axis=0)  # Concatenate all rows
frequencies, times, Sxx = spectrogram(combined_signal, fs=1.0, nperseg=256)

plt.figure(figsize=(16, 10))
plt.pcolormesh(times, frequencies, 10 * np.log10(Sxx), shading='auto', cmap='inferno')
plt.colorbar(label='Intensity (dB)')
plt.title('Combined Spectrogram of All Features')
plt.xlabel('Time (s)')
plt.ylabel('Frequency (Hz)')
plt.tight_layout()
plt.show()

def generate_wavelet_scaleogram(signal, Fs, wavelet='morl'):
    """
    Generate a wavelet scaleogram using Continuous Wavelet Transform (CWT).

    Parameters:
        signal (ndarray): 1D array containing the ECG signal.
        Fs (float): Sampling frequency (Hz).
        wavelet (str): Name of the wavelet to use (default: 'morl' for Morlet wavelet).

    Returns:
        None (Displays the scaleogram).
    """
    # Define scales for the wavelet transform
    scales = np.arange(1, 128)  # Adjust as needed for resolution

    # Compute the Continuous Wavelet Transform (CWT)
    coefficients, frequencies = pywt.cwt(signal, scales, wavelet, 1/Fs)

    # Plot the Wavelet Scaleogram
    plt.figure(figsize=(10, 6))
    plt.imshow(np.abs(coefficients), aspect='auto', extent=[0, len(signal)/Fs, 1, max(scales)], cmap='jet')
    plt.colorbar(label="Magnitude")
    plt.ylabel("Scales")
    plt.xlabel("Time (s)")
    plt.title("Wavelet Scaleogram of MIT-BIH")
    plt.gca().invert_yaxis()  # Invert y-axis for proper representation
    plt.show()

# Example Usage
def main():
    # Sampling frequency (Hz)
    Fs = 360
    # Duration of the signal in seconds
    duration = 2
    # Time vector
    t = np.linspace(0, duration, int(Fs * duration), endpoint=False)

    # Generate a synthetic ECG-like signal
    signal = np.sin(2 * np.pi * 1.2 * t) + 0.3 * np.sin(2 * np.pi * 0.25 * t) + 0.5 * np.sin(2 * np.pi * 3 * t)

    # Generate the Wavelet Scaleogram
    generate_wavelet_scaleogram(signal, Fs)

if __name__ == '__main__':
    main()

"""### ADVANCE MODERN VISUALIZATION AND DIMENTIONALITY REDUCTION

APPLYING PCA ON DATA FOR DIMENSIONAL REDUCTION
"""

# Step 3: Identify Target Variable
def identify_target(data):
    # Columns with few unique values
    candidates = data.nunique()[data.nunique() < 20].index.tolist()
    potential_targets = [col for col in candidates if data[col].dtype in ["int64", "float64", "object"]]
    print(f"Potential target columns: {potential_targets}")
    return potential_targets[0] if potential_targets else None

target_column = identify_target(data)
if not target_column:
    raise ValueError("Target variable could not be identified. Please specify manually.")

# Step 4: PCA on ECG Features
scaler = StandardScaler()
ecg_features = numeric_data
ecg_scaled = scaler.fit_transform(ecg_features)

# PCA with explained variance analysis
pca = PCA()
ecg_pca = pca.fit_transform(ecg_scaled)
explained_variance = pca.explained_variance_ratio_
cumulative_variance = explained_variance.cumsum()

# Plot cumulative explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker="o", linestyle="--", color="b")
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Explained Variance by Principal Components")
plt.grid()
plt.show()

# Choose components explaining 95% variance
n_components = next(i for i, total in enumerate(cumulative_variance) if total >= 0.95) + 1
print(f"Number of components explaining ~95% variance: {n_components}")
pca_final = PCA(n_components=n_components)
ecg_pca_final = pca_final.fit_transform(ecg_scaled)

# Save PCA-transformed data
ecg_pca_df = pd.DataFrame(ecg_pca_final, columns=[f"PC{i+1}" for i in range(n_components)])
ecg_pca_df.to_csv("ecg_pca_transformed.csv", index=False)
print("PCA-transformed ECG data saved to 'ecg_pca_transformed.csv'.")

# Take a subset for better performance
data_sampled = data.sample(n=min(5000, len(data)), random_state=42)

# Step 1: Preprocess Data
target_column = 'type'
X = data_sampled.drop(columns=[target_column])
y = data_sampled[target_column]

# Standardize Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 2: Apply PCA for Dimensionality Reduction (if necessary)
n_pca = min(30, X_scaled.shape[1])  # Auto-adjust number of components
pca = PCA(n_components=n_pca)
X_pca = pca.fit_transform(X_scaled)

# Step 3: Apply UMAP
umap_model = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.3, metric='euclidean', random_state=42)
X_umap = umap_model.fit_transform(X_pca)

# Step 4: Convert UMAP Results to DataFrame
umap_df = pd.DataFrame(X_umap, columns=['UMAP1', 'UMAP2'])
umap_df[target_column] = y.values  # Ensure correct alignment

# Step 5: Plot UMAP Visualization
plt.figure(figsize=(12, 8))
sns.scatterplot(data=umap_df, x='UMAP1', y='UMAP2', hue=target_column, palette="plasma", alpha=0.7, edgecolor=None)
plt.title("Optimized UMAP Visualization", fontsize=14)
plt.legend(title=target_column, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
plt.grid(True, linestyle="--", alpha=0.5)
plt.show()

# Take a subset for better performance
data_sampled = data.sample(n=min(5000, len(data)), random_state=42)

# Step 1: Preprocess Data
target_column = 'type'
X = data_sampled.drop(columns=[target_column])
y = data_sampled[target_column]

# Standardize Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 2: Apply PCA for Dimensionality Reduction (if necessary)
n_pca = min(30, X_scaled.shape[1])  # Auto-adjust number of components
pca = PCA(n_components=n_pca)
X_pca = pca.fit_transform(X_scaled)

# Step 3: Apply t-SNE with optimized parameters
perplexity_value = min(30, len(X_pca) - 1)  # Perplexity should be < (num_samples - 1)
tsne = TSNE(n_components=2, perplexity=perplexity_value, learning_rate=200, method='barnes_hut', random_state=42)
X_tsne = tsne.fit_transform(X_pca)

# Step 4: Convert t-SNE Results to DataFrame
tsne_df = pd.DataFrame(X_tsne, columns=['t-SNE1', 't-SNE2'])
tsne_df[target_column] = y.values  # Ensure correct alignment

# Step 5: Plot t-SNE Visualization
plt.figure(figsize=(12, 8))
sns.scatterplot(data=tsne_df, x='t-SNE1', y='t-SNE2', hue=target_column, palette="viridis", alpha=0.7, edgecolor=None)
plt.title("Optimized t-SNE Visualization", fontsize=14)
plt.legend(title=target_column, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
plt.grid(True, linestyle="--", alpha=0.5)
plt.show()

# Let's generate the time values assuming a fixed sampling rate (360 Hz in this case)
sampling_rate = 360  # Sampling rate in Hz
time = np.linspace(0, len(data) / sampling_rate, len(data))  # Create time vector based on number of rows

# Extract ECG signal columns (replace these with the actual signal columns you want to visualize)
# Let's assume you're interested in columns such as '0_pre-RR', '0_post-RR', '1_pre-RR', etc.
# You can adjust the column names based on the dataset structure.
lead1 = data['0_pre-RR'].values  # Example column, replace with actual lead data
lead2 = data['0_post-RR'].values  # Example column, replace with actual lead data
lead3 = data['1_pre-RR'].values  # Example column, replace with actual lead data

# Create a 3D plot
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')

# Plot each lead in 3D space
ax.plot(time, lead1, zs=0, zdir='y', label='Lead I', color='r')
ax.plot(time, lead2, zs=1, zdir='y', label='Lead II', color='g')
ax.plot(time, lead3, zs=2, zdir='y', label='Lead III', color='b')

# Set axis labels
ax.set_xlabel('Time (s)')
ax.set_ylabel('Leads')
ax.set_zlabel('Amplitude')

# Add a legend
ax.legend()

# Title
ax.set_title('3D ECG Signal Visualization from CSV Dataset')

# Show plot
plt.show()

"""## MACHINE LEARNING APPLYIED AND EVLUATED RESULTS - 9% (MAX ACCURATE)

APPLYING LAZYREGRESSOR TO EVALUATE BEST MODELS
"""

# -----------------------------
# 1. Load Data with More Aggressive Sampling
# -----------------------------
dataset_path = "/root/.cache/kagglehub/datasets/gkshitij/mit-bih-arrhythmia-database-new/versions/1/MIT-BIH Arrhythmia Database new.csv"

# Reduce sample fraction further (e.g., 0.001 = 0.1% sample) or use a fixed number of rows
sample_fraction = 0.1  # Adjust to a smaller fraction for speed
chunk_size = 3000  # Process data in chunks

# Read in chunks to reduce memory load and sample fewer rows
data_chunks = pd.read_csv(dataset_path, chunksize=chunk_size)
data = pd.concat([chunk.sample(frac=sample_fraction, random_state=42) for chunk in data_chunks])

print(f"Sampled Data Shape: {data.shape}")

# -----------------------------
# 2. Data Preprocessing (Assuming X_resampled and y_resampled are already defined)
# -----------------------------
# For demonstration, let's assume X_resampled and y_resampled are derived from the sampled data.
# Here we perform a simple split. Adjust based on your actual preprocessing pipeline.
# For example, assume 'type' is the target column:
X = data.drop(columns=['type'])
y = data['type']

# Optionally, apply PCA if needed (here using all data may be fast enough given the reduced sample size)
pca_full = PCA()
pca_full.fit(X)
explained_variance_ratio = pca_full.explained_variance_ratio_.cumsum()
desired_components = (explained_variance_ratio >= 0.95).argmax() + 1
print(f"Selected {desired_components} components to retain 95% variance.")

pca = PCA(n_components=desired_components)
X_pca = pca.fit_transform(X)

# -----------------------------
# 3. Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.2, random_state=42
)

# -----------------------------
# 4. LazyClassifier Evaluation
# -----------------------------
print("Performing LazyClassifier (Classification Task)...")
clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

# Ensure the "output" directory exists
os.makedirs("output", exist_ok=True)
output_file = "output/lazy_classifier_results.csv"
models.to_csv(output_file, index=True)

print("\nð LazyClassifier Results:")
print(models)  # Print results to the console
print(f"\nâ LazyClassifier results saved as '{output_file}' and displayed above.")

!pip install lightgbm

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder

# -----------------------------
# 1. Load Data with Efficient Sampling
# -----------------------------
dataset_path = "/root/.cache/kagglehub/datasets/gkshitij/mit-bih-arrhythmia-database-new/versions/1/MIT-BIH Arrhythmia Database new.csv"

# Use a small sample fraction and process the data in chunks to reduce memory load
sample_fraction = 0.01  # 1% sample per chunk
chunk_size = 3000

data_chunks = pd.read_csv(dataset_path, chunksize=chunk_size)
data = pd.concat([chunk.sample(frac=sample_fraction, random_state=42) for chunk in data_chunks])
print(f"Sampled Data Shape: {data.shape}")

# -----------------------------
# 2. Data Preprocessing
# -----------------------------
# Assume the target column is named 'type'
X = data.drop(columns=['type'])
y = data['type']

# Convert target variable to numeric labels using LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

# Apply PCA to reduce dimensionality while retaining 95% of variance
pca_full = PCA()
pca_full.fit(X)
explained_variance_ratio = np.cumsum(pca_full.explained_variance_ratio_)
desired_components = np.argmax(explained_variance_ratio >= 0.95) + 1
print(f"Selected {desired_components} components to retain 95% variance.")

pca = PCA(n_components=desired_components)
X_pca = pca.fit_transform(X)

# -----------------------------
# 3. Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# -----------------------------
# 4. Top 4 Models Implementation and Evaluation
# -----------------------------

# For semi-supervised models, simulate unlabeled data by masking 30% of training labels
rng = np.random.RandomState(42)
mask = rng.rand(len(y_train)) < 0.3  # Mask 30% of training labels
y_train_semi = y_train.copy()
y_train_semi[mask] = -1

# --- LabelPropagation ---
lp = LabelPropagation()
lp.fit(X_train, y_train_semi)
y_pred_lp = lp.predict(X_test)
print("=== LabelPropagation ===")
print("Accuracy:", accuracy_score(y_test, y_pred_lp))
print("Classification Report:\n", classification_report(y_test, y_pred_lp))

# --- LabelSpreading ---
ls = LabelSpreading()
ls.fit(X_train, y_train_semi)
y_pred_ls = ls.predict(X_test)
print("=== LabelSpreading ===")
print("Accuracy:", accuracy_score(y_test, y_pred_ls))
print("Classification Report:\n", classification_report(y_test, y_pred_ls))

# --- LGBMClassifier ---
lgbm = LGBMClassifier(random_state=42)
lgbm.fit(X_train, y_train)  # Use full training labels for supervised model
y_pred_lgbm = lgbm.predict(X_test)
print("=== LGBMClassifier ===")
print("Accuracy:", accuracy_score(y_test, y_pred_lgbm))
print("Classification Report:\n", classification_report(y_test, y_pred_lgbm))

# --- KNeighborsClassifier ---
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)  # Use full training labels for supervised model
y_pred_knn = knn.predict(X_test)
print("=== KNeighborsClassifier ===")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("Classification Report:\n", classification_report(y_test, y_pred_knn))

"""APPLYING BEST FOUR MODELS AND ANALYZING THERE RESULTS

## DEEP LEARNING CNN IMPLEMENTATION - 99.88% (ACCURACY)
"""

# Load the data
file_path = "/root/.cache/kagglehub/datasets/gkshitij/mit-bih-arrhythmia-database-new/versions/1/MIT-BIH Arrhythmia Database new.csv"  # Replace with your actual file path
data = pd.read_csv(file_path)

# Automatically select the target column
label_column = data.select_dtypes(include=['object']).columns[0]  # First column with categorical data

# If that doesn't work, you can also try selecting the last column as the label
if label_column not in data.columns:
    label_column = data.columns[-1]

# Extract features and labels
X = data.drop(columns=[label_column])  # Drop the label column
y = data[label_column]  # The label column

print(f"Label column: {label_column}")

# Adjustable sampling factor (0 < sample_factor <= 1)
sample_factor = 0.8  # Use 80% of the data for faster training, can adjust as needed

# Sample the data to reduce the dataset size for faster training
X_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_factor, random_state=42)

# Handle class imbalance using upsampling (if necessary)
X_resampled, y_resampled = resample(X_sampled, y_sampled, replace=True, n_samples=X_sampled.shape[0], random_state=42)

# Encode the labels as numeric values
label_encoder = LabelEncoder()
y_resampled_encoded = label_encoder.fit_transform(y_resampled)

# Print the unique values in y_resampled_encoded to confirm proper encoding
print(f"Encoded labels: {np.unique(y_resampled_encoded)}")

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Reshape the data to be compatible with CNN input (samples, timesteps, features)
X_scaled_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)

# Check data types
print(f"Data type of X_train: {X_scaled_reshaped.dtype}")
print(f"Data type of y_train: {y_resampled_encoded.dtype}")

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled_reshaped, y_resampled_encoded, test_size=0.2, random_state=42)

# Check if the labels are integers
print(f"y_train unique values: {np.unique(y_train)}")
print(f"y_test unique values: {np.unique(y_test)}")

# # Build a simple CNN model
# model = models.Sequential([
#     layers.Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),
#     layers.MaxPooling1D(2),
#     layers.Conv1D(128, 3, activation='relu'),
#     layers.MaxPooling1D(2),
#     layers.Flatten(),
#     layers.Dense(128, activation='relu'),
#     layers.Dense(len(np.unique(y_resampled_encoded)), activation='softmax')  # Assuming a classification task
# ])

# # Compile the model
# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# # Train the model and capture the history
# history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))

# # Evaluate the model
# test_loss, test_acc = model.evaluate(X_test, y_test)
# print(f"Test accuracy: {test_acc}")

# # Visualize the training and validation loss
# plt.figure(figsize=(12, 6))

# # Plot the loss
# plt.subplot(1, 2, 1)
# plt.plot(history.history['loss'], label='Train Loss')
# plt.plot(history.history['val_loss'], label='Validation Loss')
# plt.title('Loss Over Epochs')
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.legend()

# # Plot the accuracy
# plt.subplot(1, 2, 2)
# plt.plot(history.history['accuracy'], label='Train Accuracy')
# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
# plt.title('Accuracy Over Epochs')
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.legend()

# # Show the plots
# plt.tight_layout()
# plt.show()

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils import resample
from tensorflow.keras import models, layers, callbacks

# -----------------------------
# 1. Load & Sample the Data
# -----------------------------
file_path = "/root/.cache/kagglehub/datasets/gkshitij/mit-bih-arrhythmia-database-new/versions/1/MIT-BIH Arrhythmia Database new.csv"
data = pd.read_csv(file_path)

# Automatically pick a categorical column as label
cat_cols = data.select_dtypes(include=['object']).columns
label_column = cat_cols[0] if len(cat_cols) > 0 else data.columns[-1]
print(f"Using '{label_column}' as label.")

# Sample 80% of data for faster training
X_full = data.drop(columns=[label_column])
y_full = data[label_column]
X_sampled, _, y_sampled, _ = train_test_split(
    X_full, y_full, train_size=0.4, random_state=42, stratify=y_full
)

# Upsample to handle class imbalance (optional)
X_resampled, y_resampled = resample(
    X_sampled, y_sampled,
    replace=True,
    n_samples=len(X_sampled),
    random_state=42,
    stratify=y_sampled
)

# -----------------------------
# 2. Encode & Scale
# -----------------------------
# Encode labels to integers
le = LabelEncoder()
y_enc = le.fit_transform(y_resampled)
num_classes = len(le.classes_)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Reshape for Conv1D: (samples, timesteps, channels)
X = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)
y = y_enc

# -----------------------------
# 3. Train/Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")

# -----------------------------
# 4. Build CNN Model Factory
# -----------------------------
def create_cnn_model(dropout_rate=0.3):
    model = models.Sequential([
        layers.Conv1D(64, 3, padding='same', activation='relu', input_shape=(X_train.shape[1], 1)),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),
        layers.Dropout(dropout_rate),

        layers.Conv1D(128, 3, padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling1D(2),
        layers.Dropout(dropout_rate),

        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(dropout_rate),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# -----------------------------
# 5. Callbacks
# -----------------------------
os.makedirs("models", exist_ok=True)
checkpoint_cb = callbacks.ModelCheckpoint(
    "models/best_cnn.keras",
    monitor="val_loss",
    save_best_only=True
)
earlystop_cb = callbacks.EarlyStopping(
    monitor="val_loss",
    patience=10,
    restore_best_weights=True
)
reduce_lr_cb = callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=5,
    min_lr=1e-6
)

# -----------------------------
# 6. Train the Model
# -----------------------------
history = create_cnn_model().fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=200,
    batch_size=32,
    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],
    verbose=1
)

# -----------------------------
# 7. Evaluate on Test Set
# -----------------------------
best_model = models.load_model("models/best_cnn.keras")
test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# -----------------------------
# 8. Plot Training Curves
# -----------------------------
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()